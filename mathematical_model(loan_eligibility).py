# -*- coding: utf-8 -*-
"""MATHEMATICAL_MODEL(loan_eligibility).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19-CS7MGDaMDf6KDKUSp_PSvm6mcJcYzH
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

train_data = pd.read_csv('loan-train.csv')
test_data = pd.read_csv('loan-test.csv')

train_data.isnull().sum()
test_data.isnull().sum()

train_data = pd.get_dummies(train_data, drop_first=True)
test_data = pd.get_dummies(test_data, drop_first=True)

X_train = train_data.drop(['Loan_Status_Y'], axis=1)  # Assuming 'Loan_Status_Y' is the target
y_train = train_data['Loan_Status_Y']

# Sample training data (manually converted)
train_data = [
    [1, 0, 0, 1, 0, 5849, 0, 128, 360, 1, 1],  # Example row
    [1, 1, 1, 1, 0, 4583, 1508, 128, 360, 1, 0],
    # Add more rows...
]

train_labels = [1, 0]  # Corresponding labels for the training data

class LogisticRegression:
    def __init__(self, learning_rate=0.01, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.coefficients = None

    def sigmoid(self, z):
        if z < -709:
            return 0
        elif z > 709:
            return 1
        return 1 / (1 + self.exp(-z))

    def exp(self, x):
        return 2.718281828459045 ** x

    def fit(self, X, y):
        n_samples, n_features = len(X), len(X[0])
        self.coefficients = [0] * n_features

        for _ in range(self.num_iterations):
            linear_model = [sum(x * coef for x, coef in zip(sample, self.coefficients)) for sample in X]
            y_predicted = [self.sigmoid(x) for x in linear_model]

            for j in range(n_features):
                gradient = sum((y_predicted[i] - y[i]) * X[i][j] for i in range(n_samples)) / n_samples
                self.coefficients[j] -= self.learning_rate * gradient

    def predict(self, X):
        linear_model = [sum(x * coef for x, coef in zip(sample, self.coefficients)) for sample in X]
        y_predicted = [self.sigmoid(x) for x in linear_model]
        return [1 if i >= 0.5 else 0 for i in y_predicted]

class RandomForest:
    def __init__(self, n_trees=10):
        self.n_trees = n_trees
        self.trees = []  # Placeholder for decision trees

    def fit(self, X, y):
        # In a real implementation, you would create and fit multiple trees here
        pass

    def predict(self, X):
        return [0] * len(X)  # Predicting all zeros (placeholder)

class StackingModel:
    def __init__(self, model1, model2):
        self.model1 = model1
        self.model2 = model2

    def fit(self, X, y):
        self.model1.fit(X, y)
        self.model2.fit(X, y)

    def predict(self, X):
        preds1 = self.model1.predict(X)
        preds2 = self.model2.predict(X)

        combined_preds = [(preds1[i] + preds2[i]) / 2 for i in range(len(preds1))]
        return [1 if i >= 0.5 else 0 for i in combined_preds]

# Initialize your models
log_reg = LogisticRegression()
rand_forest = RandomForest(n_trees=10)  # Placeholder
stacking_model = StackingModel(log_reg, rand_forest)

# Fit the stacking model with the training data
stacking_model.fit(train_data, train_labels)

# Sample test data (manually converted)
test_data = [
    [1, 0, 0, 1, 0, 5720, 0, 110, 360, 1],  # Example row
    [1, 1, 1, 1, 0, 3076, 1500, 126, 360, 1],
    # Add more rows...
]

predictions = stacking_model.predict(test_data)
print(predictions)  # This will output predicted labels for the test data